<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="G2qrltqMfSqv5T0Y_vQoqywW0Y5AJDic9h8aDGgrQLU"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Gentle Introduction to LLM Architectures - Encoder, Decoder, and Encoder-Decoder Models | Yuxin Shen </title> <meta name="author" content="Yuxin Shen"> <meta name="description" content="ML blogs - Deep Learning 1"> <meta name="keywords" content="Yuxin Shen, Yuxin, Shen, Edinburgh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/autumn.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuxinshen233.github.io/blog/2025/ML4_LLM_general/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yuxin Shen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Gentle Introduction to LLM Architectures - Encoder, Decoder, and Encoder-Decoder Models</h1> <p class="post-meta"> May 26, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep learning</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/ml-learning-blogs"> <i class="fa-solid fa-tag fa-sm"></i> ML learning blogs</a> ¬† </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Large Language Models (LLMs) have become a cornerstone of modern AI, powering applications ranging from chatbots and translators to code assistants and search engines. At the heart of most LLMs lies the <strong>Transformer</strong> architecture, introduced by Vaswani et al. in 2017.</p> <p>But not all Transformer-based models are structured the same way. In this post, we‚Äôll outline the three main types of Transformer architectures:</p> <ul> <li><strong>Encoder-only</strong></li> <li><strong>Decoder-only</strong></li> <li><strong>Encoder-Decoder</strong></li> </ul> <p>Understanding these structures helps explain why some models excel at classification, while others are better suited for generation or translation.</p> <h1 id="-encoder-only-models">üîç Encoder-only Models</h1> <p>Encoder-only models process input sequences to generate contextualized representations. These are typically used for <strong>understanding tasks</strong>, such as:</p> <ul> <li>Sentence classification</li> <li>Named entity recognition (NER)</li> <li>Embedding generation (e.g., for retrieval)</li> </ul> <h3 id="structure">Structure</h3> <ul> <li>The entire input sequence is processed simultaneously.</li> <li>Each token attends to all others (bidirectional attention).</li> <li>Outputs are contextual embeddings for each token or the full sentence.</li> </ul> <h3 id="examples">Examples</h3> <ul> <li> <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers)</li> <li> <strong>RoBERTa</strong>, <strong>DistilBERT</strong>, <strong>E5</strong> </li> </ul> <h3 id="use-cases">Use Cases</h3> <ul> <li>Semantic search</li> <li>Sentiment analysis</li> <li>Question answering (retrieval-based)</li> </ul> <h1 id="Ô∏è-decoder-only-models">‚úçÔ∏è Decoder-only Models</h1> <p>Decoder-only models are <strong>causal</strong>: they generate text one token at a time, attending only to past tokens (left-to-right). This makes them ideal for <strong>language modeling and text generation</strong>.</p> <h3 id="structure-1">Structure</h3> <ul> <li>Input is processed autoregressively (each token only sees previous tokens).</li> <li>Only self-attention is used (no separate encoder).</li> </ul> <h3 id="examples-1">Examples</h3> <ul> <li> <strong>GPT</strong> (Generative Pretrained Transformer) family</li> <li> <strong>LLaMA</strong>, <strong>Mistral</strong>, <strong>Gemma</strong> </li> </ul> <h3 id="use-cases-1">Use Cases</h3> <ul> <li>Text generation</li> <li>Code synthesis</li> <li>Autocomplete</li> <li>Instruction-following (e.g., ChatGPT)</li> </ul> <h1 id="-encoder-decoder-models">üîÅ Encoder-Decoder Models</h1> <p>Encoder-decoder models are often used in <strong>sequence-to-sequence tasks</strong>, where input and output are both important but distinct. Think of translating a sentence from English to French: the input must be understood, and a new sequence must be generated.</p> <h3 id="structure-2">Structure</h3> <ul> <li>The <strong>encoder</strong> processes the input sequence into a set of contextual embeddings.</li> <li>The <strong>decoder</strong> generates the output sequence, attending to both previous output tokens (causal) and the encoder output (cross-attention).</li> </ul> <h3 id="examples-2">Examples</h3> <ul> <li> <strong>T5</strong> (Text-To-Text Transfer Transformer)</li> <li><strong>BART</strong></li> <li> <strong>mT5</strong>, <strong>MarianMT</strong> </li> </ul> <h3 id="use-cases-2">Use Cases</h3> <ul> <li>Machine translation</li> <li>Summarization</li> <li>Text rewriting</li> <li>Instruction-based generation (with better controllability)</li> </ul> <p>Choosing the right architecture depends on the task. For embedding-based retrieval, encoder-only works best. For free-form generation, decoder-only models dominate. For structured input-output tasks like translation or summarization, encoder-decoder models shine.</p> <hr> <h2 id="some-questions">Some Questions</h2> <h4 id="1-why-does-chatgpt-and-most-llms-like-gpt-34-claude-llama-etc-use-decoder-only-models-instead">1. Why does ChatGPT (and most LLMs like GPT-3/4, Claude, LLaMA, etc.) use decoder-only models instead?</h4> <p>Encoder-decoder models like T5, BART, and mT5 work very well on QA tasks ‚Äî especially in supervised settings. But these models are less commonly used for interactive generation tasks like chat, because:</p> <ul> <li>They need to encode the input first before decoding, making training less efficient.</li> <li>They aren‚Äôt naturally autoregressive across an open-ended conversation.</li> </ul> <h4 id="2-how-are-these-models-trained">2. How are these models trained?</h4> <ul> <li>Encoder-only: Mask language model traning (bidirectional), then add classification heads for downstream training tasks.</li> <li>Decoder-only: Causal Language Modeling (Autoregressive), then train on (instruction, output) pairs or use RLHF.</li> <li>Encoder-Decoder: Train the Encoder with MLM, the Decoder with CLM, and then train using labeled (input, output) examples.</li> </ul> <p>We will dive into the structure and training of the models in more detail from the next post!</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Yuxin Shen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?4a94ac0b541c11c7902119bd08161982"></script> <script defer src="/assets/js/common.js?a1fc0620211583d04aea7b6a840f1ea7"></script> <script defer src="/assets/js/copy_code.js?f92f864e1a83c41acfbb9e3d94571f90" type="text/javascript"></script> <script defer src="/assets/js/navbar_underline.js"></script> <script defer src="/assets/js/mathjax_config.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>