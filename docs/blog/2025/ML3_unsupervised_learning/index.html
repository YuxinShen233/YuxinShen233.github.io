<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="G2qrltqMfSqv5T0Y_vQoqywW0Y5AJDic9h8aDGgrQLU"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unsupervised learning - Dimensionality reduction and clustering | Yuxin Shen </title> <meta name="author" content="Yuxin Shen"> <meta name="description" content="ML blogs - Classical ML 3"> <meta name="keywords" content="Yuxin Shen, Yuxin, Shen, Edinburgh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/autumn.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuxinshen233.github.io/blog/2025/ML3_unsupervised_learning/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yuxin Shen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unsupervised learning - Dimensionality reduction and clustering</h1> <p class="post-meta"> May 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/classical-ml"> <i class="fa-solid fa-hashtag fa-sm"></i> Classical ML</a>     ·   <a href="/blog/category/ml-learning-blogs"> <i class="fa-solid fa-tag fa-sm"></i> ML learning blogs</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Unsupervised learning forms an important part of Machine learning. On unlabeled data, the main task is to find uncovering hidden patterns, structures, or groupings.</p> <h2 id="what-are-the-unsupervised-learning-tasks">What are the unsupervised learning tasks?</h2> <p><strong>1. Density Estimation</strong><br> Estimate the underlying probability distribution of the data.<br> Examples:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">KDE</code>, <code class="language-plaintext highlighter-rouge">GMM</code> </li> <li>Applications: anomaly detection, generative modeling</li> </ul> <p><strong>2. Clustering</strong><br> Group similar data points based on feature similarity.<br> Examples:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">KMeans</code>, <code class="language-plaintext highlighter-rouge">DBSCAN</code>, <code class="language-plaintext highlighter-rouge">Agglomerative Clustering</code> </li> <li>Applications: customer segmentation, image grouping</li> </ul> <p><strong>3. Dimensionality Reduction</strong><br> Reduce the number of input features while preserving important structure in the data.<br> Examples:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">PCA</code>, <code class="language-plaintext highlighter-rouge">t-SNE</code>, <code class="language-plaintext highlighter-rouge">UMAP</code> </li> <li>Applications: data visualization, feature extraction</li> </ul> <h2 id="1-density-estimation">1. Density estimation</h2> <p><strong>Parametric methods</strong> - Assume the data follows a known distribution form (e.g. Gaussian) and fit parameters.</p> <ul> <li>Gaussian Mixture Models (GMMs): A flexible model that represents the data as a mixture of multiple Gaussians. Each component has its own mean and covariance, allowing the model to approximate complex shapes. GMMs assume that the data is generated from a mixture of several Gaussian distributions, which may not always be the case in real-world scenarios.</li> </ul> <p><strong>Non-parametric methods</strong> - Make fewer assumptions about the data distribution.</p> <ul> <li>Kernel Density Estimation (KDE): KDE places a kernel (e.g. a Gaussian bump) at each data point and sums them to get a smooth estimate of the density function. The bandwidth (smoothing parameter) controls how spiky or smooth the estimate is.</li> </ul> <h2 id="2-clustering-algorithm-and-evaluation">2. Clustering: algorithm and evaluation</h2> <h3 id="evaluation-of-clustering">Evaluation of clustering</h3> <p>Unlike supervised learning, we don’t have true labels. Evaluation metrics include:</p> <ul> <li> <strong>Internal</strong>: Silhouette score, Davies-Bouldin index</li> <li> <strong>External</strong> (when ground truth exists): Adjusted Rand Index, Mutual Information</li> </ul> <h3 id="why-dbscan-is-so-powerful">Why DBSCAN is so powerful?</h3> <p><img src="/assets/img/ML3-clustering.png" width="75%"></p> <p>(<a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py" rel="external nofollow noopener" target="_blank">Scikit-learn documentation</a> shows a comparison of different clustering methods.)</p> <p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise):</p> <ul> <li>Can find arbitrarily shaped clusters</li> <li>Automatically detects outliers as noise</li> <li>Does not require the number of clusters to be specified in advance</li> <li>Especially useful in real-world noisy data with non-spherical structures</li> </ul> <h2 id="3-dimensionality-reduction-are-they-real-clusters">3. Dimensionality reduction: Are they real clusters?</h2> <p>Dimensionality reduction techniques like PCA, t-SNE, and UMAP are widely used to project high-dimensional data into 2D or 3D space for visualization. These methods can help uncover apparent clusters or structures. However, it’s important to remember that not all visual clusters represent true separations or clustering in the original space.</p> <h3 id="pca">PCA</h3> <p><strong>Principal Component Analysis (PCA)</strong> is a linear dimensionality reduction technique that projects the data onto the directions of maximum variance. It assumes that the most informative structure in the data can be captured through orthogonal linear combinations of the input features.</p> <p>PCA is closely related to <strong>Singular Value Decomposition (SVD)</strong> in linear algebra.</p> <h3 id="t-sne-and-umap">t-SNE and UMAP</h3> <p>When linear projections like PCA are not enough to separate complex patterns, <strong>nonlinear techniques</strong> like <strong>t-SNE</strong> and <strong>UMAP</strong> come into play.</p> <ul> <li> <strong>t-SNE (t-distributed Stochastic Neighbor Embedding)</strong>: <ul> <li>Focuses on preserving <strong>local structure</strong> </li> <li>Uses pairwise similarities and minimizes KL divergence</li> <li>Often produces visually compelling clusters, but does <strong>not preserve global distances</strong> </li> </ul> </li> <li> <strong>UMAP (Uniform Manifold Approximation and Projection)</strong>: <ul> <li>Based on manifold learning and algebraic topology</li> <li>Preserves more <strong>global structure</strong> than t-SNE</li> <li> <strong>Faster</strong> and <strong>more scalable</strong> </li> <li>Generalizes well to new data</li> </ul> </li> </ul> <h3 id="a-deeper-dive-into-the-umap-method">A deeper dive into the UMAP method</h3> <p>UMAP combines manifold learning, algebraic topology, and optimization into a powerful visualization tool.</p> <p>UMAP is built on a few key mathematical ideas:</p> <ol> <li> <p><strong>Manifold Assumption</strong><br> Assumes data lies on a low-dimensional manifold embedded in high-dimensional space.</p> </li> <li> <p><strong>Fuzzy Topological Representation</strong><br> Constructs a fuzzy simplicial set (probabilistic k-NN graph) to represent high-dimensional relationships.</p> </li> <li> <p><strong>Cross-Entropy Optimization</strong><br> Learns a low-dimensional embedding by minimizing the cross-entropy between high- and low-dimensional graphs.</p> </li> </ol> <h4 id="user-comment">User comment</h4> <p>While we know that UMAP has some randomness controlled by the random seeds, I find out that points that are exactly the same in the high dimensional space of a batch may appear to be different in the low dimensional representaion space. This is because the inherent stochasticity and numerical issues of the UMAP calculation.</p> <h4 id="reference">Reference</h4> <p>[1] McInnes, Leland, John Healy, and James Melville. “Umap: Uniform manifold approximation and projection for dimension reduction.” arXiv preprint arXiv:1802.03426 (2018).</p> <p>[2] <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py" rel="external nofollow noopener" target="_blank">Scikit-learn documentation</a></p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuxin Shen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?4a94ac0b541c11c7902119bd08161982"></script> <script defer src="/assets/js/common.js?a1fc0620211583d04aea7b6a840f1ea7"></script> <script defer src="/assets/js/copy_code.js?f92f864e1a83c41acfbb9e3d94571f90" type="text/javascript"></script> <script defer src="/assets/js/navbar_underline.js"></script> <script defer src="/assets/js/mathjax_config.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>