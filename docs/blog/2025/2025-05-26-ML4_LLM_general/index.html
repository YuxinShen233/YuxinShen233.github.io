<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h1 id="a-gentle-introduction-to-llm-architectures-encoder-decoder-and-encoder-decoder-models">A Gentle Introduction to LLM Architectures: Encoder, Decoder, and Encoder-Decoder Models</h1> <p>Large Language Models (LLMs) have become a cornerstone of modern AI, powering applications ranging from chatbots and translators to code assistants and search engines. At the heart of most LLMs lies the <strong>Transformer</strong> architecture, introduced by Vaswani et al. in 2017.</p> <p>But not all Transformer-based models are structured the same way. In this post, we‚Äôll outline the three main types of Transformer architectures:</p> <ul> <li><strong>Encoder-only</strong></li> <li><strong>Decoder-only</strong></li> <li><strong>Encoder-Decoder</strong></li> </ul> <p>Understanding these structures helps explain why some models excel at classification, while others are better suited for generation or translation.</p> <h1 id="-encoder-only-models">üîç Encoder-only Models</h1> <p>Encoder-only models process input sequences to generate contextualized representations. These are typically used for <strong>understanding tasks</strong>, such as:</p> <ul> <li>Sentence classification</li> <li>Named entity recognition (NER)</li> <li>Embedding generation (e.g., for retrieval)</li> </ul> <h3 id="structure">Structure</h3> <ul> <li>The entire input sequence is processed simultaneously.</li> <li>Each token attends to all others (bidirectional attention).</li> <li>Outputs are contextual embeddings for each token or the full sentence.</li> </ul> <h3 id="examples">Examples</h3> <ul> <li> <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers)</li> <li> <strong>RoBERTa</strong>, <strong>DistilBERT</strong>, <strong>E5</strong> </li> </ul> <h3 id="use-cases">Use Cases</h3> <ul> <li>Semantic search</li> <li>Sentiment analysis</li> <li>Question answering (retrieval-based)</li> </ul> <h1 id="Ô∏è-decoder-only-models">‚úçÔ∏è Decoder-only Models</h1> <p>Decoder-only models are <strong>causal</strong>: they generate text one token at a time, attending only to past tokens (left-to-right). This makes them ideal for <strong>language modeling and text generation</strong>.</p> <h3 id="structure-1">Structure</h3> <ul> <li>Input is processed autoregressively (each token only sees previous tokens).</li> <li>Only self-attention is used (no separate encoder).</li> </ul> <h3 id="examples-1">Examples</h3> <ul> <li> <strong>GPT</strong> (Generative Pretrained Transformer) family</li> <li> <strong>LLaMA</strong>, <strong>Mistral</strong>, <strong>Gemma</strong> </li> </ul> <h3 id="use-cases-1">Use Cases</h3> <ul> <li>Text generation</li> <li>Code synthesis</li> <li>Autocomplete</li> <li>Instruction-following (e.g., ChatGPT)</li> </ul> <h1 id="-encoder-decoder-models">üîÅ Encoder-Decoder Models</h1> <p>Encoder-decoder models are often used in <strong>sequence-to-sequence tasks</strong>, where input and output are both important but distinct. Think of translating a sentence from English to French: the input must be understood, and a new sequence must be generated.</p> <h3 id="structure-2">Structure</h3> <ul> <li>The <strong>encoder</strong> processes the input sequence into a set of contextual embeddings.</li> <li>The <strong>decoder</strong> generates the output sequence, attending to both previous output tokens (causal) and the encoder output (cross-attention).</li> </ul> <h3 id="examples-2">Examples</h3> <ul> <li> <strong>T5</strong> (Text-To-Text Transfer Transformer)</li> <li><strong>BART</strong></li> <li> <strong>mT5</strong>, <strong>MarianMT</strong> </li> </ul> <h3 id="use-cases-2">Use Cases</h3> <ul> <li>Machine translation</li> <li>Summarization</li> <li>Text rewriting</li> <li>Instruction-based generation (with better controllability)</li> </ul> <p>Choosing the right architecture depends on the task. For embedding-based retrieval, encoder-only works best. For free-form generation, decoder-only models dominate. For structured input-output tasks like translation or summarization, encoder-decoder models shine.</p> <hr> <h2 id="some-questions">Some Questions</h2> <h3 id="1-why-does-chatgpt-and-most-llms-like-gpt-34-claude-llama-etc-use-decoder-only-models-instead">1. Why does ChatGPT (and most LLMs like GPT-3/4, Claude, LLaMA, etc.) use decoder-only models instead?</h3> <p>Encoder-decoder models like T5, BART, and mT5 work very well on QA tasks ‚Äî especially in supervised settings. But these models are less commonly used for interactive generation tasks like chat, because:</p> <ul> <li>They need to encode the input first before decoding, making training less efficient.</li> <li>They aren‚Äôt naturally autoregressive across an open-ended conversation.</li> </ul> <h3 id="2-how-are-these-models-trained">2. How are these models trained?</h3> <ul> <li>Encoder-only: Mask language model traning (bidirectional), then add classification heads for downstream training tasks.</li> <li>Decoder-only: Causal Language Modeling (Autoregressive), then train on (instruction, output) pairs or use RLHF.</li> <li>Encoder-Decoder: Train the Encoder with MLM, the Decoder with CLM, and then train using labeled (input, output) examples.</li> </ul> <p>We will dive into the structure and training of the models in more detail from the next post!</p> </body></html>