<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="G2qrltqMfSqv5T0Y_vQoqywW0Y5AJDic9h8aDGgrQLU"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Evolving Memory - Understanding the "Hidden State" in RNNs, LSTMs, and Transformers | Yuxin Shen </title> <meta name="author" content="Yuxin Shen"> <meta name="description" content="ML blogs - Deep Learning 2"> <meta name="keywords" content="Yuxin Shen, Yuxin, Shen, Edinburgh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/autumn.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuxinshen233.github.io/blog/2025/ML5_hidden_states/"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/monokai.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Yuxin Shen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Evolving Memory - Understanding the "Hidden State" in RNNs, LSTMs, and Transformers</h1> <p class="post-meta"> June 29, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep learning</a>     ·   <a href="/blog/category/ml-learning-blogs"> <i class="fa-solid fa-tag fa-sm"></i> ML learning blogs</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In sequence machine learning models (RNNs, LSTMs, and Transformers), it is sometimes confusing to talk about “hidden states” because they refer to different structures in the model. In this blog, I would like to compare the concept of <strong>hidden state</strong> in different models, in order to achieve a better understanding of the advantage and limitation of different models.</p> <h2 id="rnn-the-simple-recurrent-memory">RNN: The Simple, Recurrent Memory</h2> <p>In Recurrent Neural Networks (RNN), the hidden state is a straightforward and intuitive concept. It is a vector that encapsulates a summary of the information from all previous time steps in a sequence. At each step, the RNN’s hidden state is updated based on two inputs: the current input element of the sequence and the hidden state from the previous time step.</p> <p><img src="/assets/img/ML5_rnn.jpg" width="75%"> (Figure 1: RNN structure. Taken from Chapter 10 of the book[1].)</p> <p>This process can be visualized as a loop where the network’s output from one step is fed back into itself for the next. The mathematical representation is a simple function:</p> \[h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\] <p>where:</p> <ul> <li>$h_t$ is the new hidden state at time step <em>t</em>.</li> <li>$h_{t-1}$ is the hidden state from the previous time step <em>t-1</em>.</li> <li>$x_t$ is the input at the current time step <em>t</em>.</li> <li>$W_{hh}$ and $W_{xh}$ are weight matrices.</li> <li>$b_h$ is a bias vector.</li> <li> <code class="language-plaintext highlighter-rouge">tanh</code> is the activation function.</li> </ul> <p>This recurrent nature allows the RNN to maintain a memory of the past. However, this simple architecture suffers from the <strong>“vanishing gradient problem,”</strong> where the influence of earlier time steps diminishes rapidly over long sequences, making it difficult for the model to learn long-range dependencies. This is solved by the following LSTM structure.</p> <h2 id="lstm-a-gated-and-more-sophisticated-memory">LSTM: A Gated and More Sophisticated Memory</h2> <p>Long Short-Term Memory (LSTM) networks were specifically designed to address the shortcomings of simple RNNs. LSTMs introduce a more complex internal structure within each recurrent cell, featuring a dual-state memory system and a series of <strong>“gates”</strong> that regulate the flow of information.</p> <p><img src="/assets/img/ML5_lstm.jpg" width="75%"> (Figure 2: LSTM structure. Taken from Chapter 10 of the book[1].)</p> <p>In an LSTM, the “hidden state” is split into two components:</p> <ul> <li> <strong>Cell State ($c_t$):</strong> This acts as the <em>long-term memory</em> of the network. It can carry information over many time steps with minimal distortion.</li> <li> <strong>Hidden State ($h_t$):</strong> This is often referred to as the <em>short-term memory</em> or the “output” hidden state. It is a filtered version of the cell state and is what is passed on to the next layer or used for predictions at the current time step.</li> </ul> <p>The key innovation of LSTMs lies in the <strong>gates</strong>, which are neural networks in themselves that learn to control what information is added to or removed from the cell state. There are three main gates:</p> <ol> <li> <strong>Forget Gate:</strong> Decides what information from the previous cell state ($c_{t-1}$) should be discarded.</li> <li> <strong>Input Gate:</strong> Determines which new information from the current input ($x_t$) and previous hidden state ($h_{t-1}$) should be stored in the current cell state.</li> <li> <strong>Output Gate:</strong> Controls what information from the current cell state ($c_t$) is passed on to the new hidden state ($h_t$).</li> </ol> <p>This gating mechanism allows LSTMs to selectively remember or forget information, enabling them to capture and maintain long-range dependencies in data much more effectively than simple RNNs.</p> <p><strong>To conclude, hidden state in RNN and LSTM means the sequential information passing between different time steps.</strong></p> <h2 id="transformer-a-parallel-and-contextual-representation">Transformer: A Parallel and Contextual Representation</h2> <p>The Transformer architecture, introduced in the paper “Attention Is All You Need,”[2] marked a paradigm shift. It completely dispenses with the recurrent structure of RNNs and LSTMs. Instead of processing a sequence step-by-step and maintaining a hidden state that evolves over time, the Transformer processes the entire input sequence at once.</p> <p>In a Transformer, the concept of a single, evolving hidden state is replaced by a set of <strong>contextualized embeddings</strong>. The term “hidden state” in the context of a Transformer typically refers to the output vector for each token at each layer of the model.</p> <p>Here’s how it works:</p> <ol> <li> <p><strong>Input Embeddings and Positional Encodings:</strong> The input sequence is first converted into a series of embeddings. This embedding is the initial, shallow representation of an input token (a word or sub-word). It is the first step in converting text into a numerical format that the model can process. Since the model doesn’t have a recurrent structure to understand the order of the tokens, <strong>positional encodings</strong> are added to these embeddings to provide the model with information about the position of each token in the sequence.</p> </li> <li> <p><strong>Self-Attention:</strong> The core of the Transformer is the <strong>self-attention mechanism</strong>. For each token in the sequence, self-attention allows the model to weigh the importance of all other tokens in the sequence when creating a new representation for that token. This means the representation of a word is not just based on the word itself but on its relationships with every other word in the sentence.</p> </li> <li> <p><strong>Multi-Layer Architecture:</strong> A Transformer is composed of a stack of identical encoder or decoder layers. The output of one layer (a set of contextualized embeddings for each token) serves as the input to the next layer. The “hidden state” of a Transformer can be seen as the output of any of these layers. The final layer’s hidden states provide the most contextually rich representations of the input tokens.</p> </li> </ol> <p>In essence, while an RNN’s hidden state is a summary of the past, a Transformer’s “hidden state” for a particular token is a rich, contextualized representation of that token, informed by the entire input sequence simultaneously. This parallel processing capability and the ability to capture complex relationships between all tokens have made Transformers the dominant architecture for a wide range of natural language processing tasks.</p> <h2 id="summary">Summary</h2> <p>The evolution from RNNs to LSTMs and then to Transformers is a story of developing more sophisticated and effective memory mechanisms.</p> <ul> <li>The <strong>RNN’s hidden state</strong> is a simple, sequential memory that struggles with long-term dependencies.</li> <li>The <strong>LSTM’s hidden and cell states</strong>, regulated by gates, provide a more robust and controlled memory, capable of retaining information over longer sequences.</li> <li>The <strong>Transformer’s “hidden state”</strong>, in the form of contextualized embeddings generated through self-attention, represents a move away from sequential memory to a parallel, holistic understanding of the entire input sequence at once. This has unlocked new levels of performance and efficiency in processing sequential data.</li> </ul> <p>We will detail the Transformer structure, with its positional encoding and attention mechanism in the next series of posts!</p> <h3 id="reference-and-figure-source">Reference and Figure source:</h3> <p>[1] Goodfellow, Ian, et al. Deep learning. Vol. 1. No. 2. Cambridge: MIT press, 2016.</p> <p>[2] Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).</p> <p>[3] StackExchange https://datascience.stackexchange.com/questions/82808/whats-the-difference-between-the-cell-and-hidden-state-in-lstm</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuxin Shen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?4a94ac0b541c11c7902119bd08161982"></script> <script defer src="/assets/js/common.js?a1fc0620211583d04aea7b6a840f1ea7"></script> <script defer src="/assets/js/copy_code.js?f92f864e1a83c41acfbb9e3d94571f90" type="text/javascript"></script> <script defer src="/assets/js/navbar_underline.js"></script> <script defer src="/assets/js/mathjax_config.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script defer id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> </body> </html>