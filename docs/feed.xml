<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://yuxinshen233.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yuxinshen233.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-17T16:49:22+01:00</updated><id>https://yuxinshen233.github.io/feed.xml</id><title type="html">Yuxin Shen</title><subtitle>Personal website of Yuxin Shen. </subtitle><entry><title type="html">Unsupervised learning - Dimensionality reduction and clustering</title><link href="https://yuxinshen233.github.io/blog/2025/ML3_unsupervised_learning/" rel="alternate" type="text/html" title="Unsupervised learning - Dimensionality reduction and clustering"/><published>2025-05-16T00:00:00+01:00</published><updated>2025-05-16T00:00:00+01:00</updated><id>https://yuxinshen233.github.io/blog/2025/ML3_unsupervised_learning</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2025/ML3_unsupervised_learning/"><![CDATA[<p>Unsupervised learning forms an important part of Machine learning. On unlabeled data, the main task is to find uncovering hidden patterns, structures, or groupings.</p> <h2 id="what-are-the-unsupervised-learning-tasks">What are the unsupervised learning tasks?</h2> <p><strong>1. Density Estimation</strong><br/> Estimate the underlying probability distribution of the data.<br/> Examples:</p> <ul> <li><code class="language-plaintext highlighter-rouge">KDE</code>, <code class="language-plaintext highlighter-rouge">GMM</code></li> <li>Applications: anomaly detection, generative modeling</li> </ul> <p><strong>2. Clustering</strong><br/> Group similar data points based on feature similarity.<br/> Examples:</p> <ul> <li><code class="language-plaintext highlighter-rouge">KMeans</code>, <code class="language-plaintext highlighter-rouge">DBSCAN</code>, <code class="language-plaintext highlighter-rouge">Agglomerative Clustering</code></li> <li>Applications: customer segmentation, image grouping</li> </ul> <p><strong>3. Dimensionality Reduction</strong><br/> Reduce the number of input features while preserving important structure in the data.<br/> Examples:</p> <ul> <li><code class="language-plaintext highlighter-rouge">PCA</code>, <code class="language-plaintext highlighter-rouge">t-SNE</code>, <code class="language-plaintext highlighter-rouge">UMAP</code></li> <li>Applications: data visualization, feature extraction</li> </ul> <h2 id="1-density-estimation">1. Density estimation</h2> <p><strong>Parametric methods</strong> - Assume the data follows a known distribution form (e.g. Gaussian) and fit parameters.</p> <ul> <li>Gaussian Mixture Models (GMMs): A flexible model that represents the data as a mixture of multiple Gaussians. Each component has its own mean and covariance, allowing the model to approximate complex shapes.</li> </ul> <p><strong>Non-parametric methods</strong> - Make fewer assumptions about the data distribution.</p> <ul> <li>Kernel Density Estimation (KDE): KDE places a kernel (e.g. a Gaussian bump) at each data point and sums them to get a smooth estimate of the density function. The bandwidth (smoothing parameter) controls how spiky or smooth the estimate is.</li> </ul> <h2 id="2-clustering-algorithm-and-evaluation">2. Clustering: algorithm and evaluation</h2> <h3 id="evaluation-of-clustering">Evaluation of clustering</h3> <p>Unlike supervised learning, we don’t have true labels. Evaluation metrics include:</p> <ul> <li><strong>Internal</strong>: Silhouette score, Davies-Bouldin index</li> <li><strong>External</strong> (when ground truth exists): Adjusted Rand Index, Mutual Information</li> </ul> <h3 id="why-dbscan-is-so-powerful">Why DBSCAN is so powerful?</h3> <p><img src="/assets/img/ML3-clustering.png" width="75%"/></p> <p>(<a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py">Scikit-learn documentation</a> shows a comparison of different clustering methods.)</p> <p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise):</p> <ul> <li>Can find arbitrarily shaped clusters</li> <li>Automatically detects outliers as noise</li> <li>Does not require the number of clusters to be specified in advance</li> <li>Especially useful in real-world noisy data with non-spherical structures</li> </ul> <h2 id="3-dimensionality-reduction-are-they-real-clusters">3. Dimensionality reduction: Are they real clusters?</h2> <p>Dimensionality reduction techniques like PCA, t-SNE, and UMAP are widely used to project high-dimensional data into 2D or 3D space for visualization. These methods can help uncover apparent clusters or structures. However, it’s important to remember that not all visual clusters represent true separations or clustering in the original space.</p> <h3 id="pca">PCA</h3> <p><strong>Principal Component Analysis (PCA)</strong> is a linear dimensionality reduction technique that projects the data onto the directions of maximum variance. It assumes that the most informative structure in the data can be captured through orthogonal linear combinations of the input features.</p> <p>PCA is closely related to <strong>Singular Value Decomposition (SVD)</strong> in linear algebra.</p> <h3 id="t-sne-and-umap">t-SNE and UMAP</h3> <p>When linear projections like PCA are not enough to separate complex patterns, <strong>nonlinear techniques</strong> like <strong>t-SNE</strong> and <strong>UMAP</strong> come into play.</p> <ul> <li><strong>t-SNE (t-distributed Stochastic Neighbor Embedding)</strong>: <ul> <li>Focuses on preserving <strong>local structure</strong></li> <li>Uses pairwise similarities and minimizes KL divergence</li> <li>Often produces visually compelling clusters, but does <strong>not preserve global distances</strong></li> </ul> </li> <li><strong>UMAP (Uniform Manifold Approximation and Projection)</strong>: <ul> <li>Based on manifold learning and algebraic topology</li> <li>Preserves more <strong>global structure</strong> than t-SNE</li> <li><strong>Faster</strong> and <strong>more scalable</strong></li> <li>Generalizes well to new data</li> </ul> </li> </ul> <h3 id="a-deeper-dive-into-the-umap-paper---enjoy-some-maths">A deeper dive into the UMAP paper - enjoy some maths!</h3> <p>UMAP combines manifold learning, algebraic topology, and optimization into a powerful visualization tool.</p> <p>UMAP is built on a few key mathematical ideas:</p> <ol> <li> <p><strong>Manifold Assumption</strong><br/> Assumes data lies on a low-dimensional manifold embedded in high-dimensional space.</p> </li> <li> <p><strong>Fuzzy Topological Representation</strong><br/> Constructs a fuzzy simplicial set (probabilistic k-NN graph) to represent high-dimensional relationships.</p> </li> <li> <p><strong>Cross-Entropy Optimization</strong><br/> Learns a low-dimensional embedding by minimizing the cross-entropy between high- and low-dimensional graphs.</p> </li> </ol> <h4 id="reference">Reference</h4> <p>[1] McInnes, Leland, John Healy, and James Melville. “Umap: Uniform manifold approximation and projection for dimension reduction.” arXiv preprint arXiv:1802.03426 (2018).</p> <p>[2] <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py">Scikit-learn documentation</a></p>]]></content><author><name></name></author><category term="ML learning blogs"/><category term="Classical ML"/><summary type="html"><![CDATA[ML blogs - Classical ML 3]]></summary></entry><entry><title type="html">Tree-Based &amp;amp; Ensemble Models in Machine Learning</title><link href="https://yuxinshen233.github.io/blog/2025/ML2_tree_ensemble_model-copy/" rel="alternate" type="text/html" title="Tree-Based &amp;amp; Ensemble Models in Machine Learning"/><published>2025-04-05T00:00:00+01:00</published><updated>2025-04-05T00:00:00+01:00</updated><id>https://yuxinshen233.github.io/blog/2025/ML2_tree_ensemble_model%20copy</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2025/ML2_tree_ensemble_model-copy/"><![CDATA[<p>Tree-based models remain a cornerstone of classical machine learning, especially for structured data. They’re simple, intuitive, versatile, useful for interpretation, and frequently used as reliable baselines—or even final models—in both industry and research settings.</p> <h3 id="what-are-tree-based-models">What are Tree-Based Models?</h3> <p>Tree-based models use a flowchart-like structure where data is split based on feature values to make predictions. Each internal node represents a decision on a feature, and each leaf node represents an output (e.g., a class label or regression value). Common examples include <strong>Decision Trees</strong>, <strong>Random Forests</strong>, and <strong>Gradient Boosted Trees</strong>. Among them, <strong>Random Forests</strong>, and <strong>Gradient Boosted Trees</strong> are ensembles of <strong>Decision Trees</strong>.</p> <h3 id="why-tree-based-models">Why Tree-Based Models?</h3> <ul> <li>Handle both numerical and categorical features</li> <li>Require minimal preprocessing (e.g., no need for feature scaling/normalization)</li> <li>Capture non-linear relationships and feature interactions</li> <li>Offer interpretable results (feature importance, decision paths)</li> </ul> <h3 id="ensemble-models-boosting-vs-bagging">Ensemble Models: Boosting vs Bagging</h3> <p><strong>Bagging</strong> (e.g. Random Forest):</p> <ul> <li>Bootstrap aggregation, or <strong>bagging</strong>, is a general-purpose procedure for reducing the bagging variance</li> <li>Averaging many deep decision trees trained on bootstrap samples, efficiently reduces overfitting</li> <li>Random forests reduce the similarity of decision trees in the ensemble by forcing each split to consider only a subset of the predictors</li> </ul> <p><strong>Boosting</strong> (e.g. AdaBoost, XGBoost, LightGBM, CatBoost):</p> <ul> <li>Builds trees sequentially, correcting errors from previous ones</li> <li>Can achieve state-of-the-art performance on many benchmarks</li> <li>Often more sensitive to hyperparameters</li> </ul> <h3 id="how-to-get-feature-importance">How to get feature importance?</h3> <p><strong>1. From model itself</strong></p> <ul> <li>Use built-in attributes of the models (e.g., Gini importance, gain)</li> <li>Available in libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code>, <code class="language-plaintext highlighter-rouge">XGBoost</code>, and <code class="language-plaintext highlighter-rouge">LightGBM</code></li> </ul> <p><strong>2. Data-driven methods</strong></p> <ul> <li>Use permutation importance or model-agnostic tools like <code class="language-plaintext highlighter-rouge">SHAP</code></li> <li>Provide more consistent and interpretable estimates</li> </ul> <h3 id="when-to-use-tree-based-models">When to use Tree-based models?</h3> <ul> <li>On small to medium-sized tabular datasets</li> <li>For feature importance and model interpretability</li> <li>As strong baselines before trying more complex deep models</li> </ul> <h3 id="useful-tools--libraries">Useful Tools &amp; Libraries:</h3> <ul> <li><code class="language-plaintext highlighter-rouge">scikit-learn</code>: Simple, solid implementations of decision trees and random forests, integrated with feature importance calculation (entropy of node splitting)</li> <li><code class="language-plaintext highlighter-rouge">XGBoost</code>, <code class="language-plaintext highlighter-rouge">LightGBM</code>, <code class="language-plaintext highlighter-rouge">CatBoost</code>: Optimized gradient boosting libraries with great performance and features</li> <li><code class="language-plaintext highlighter-rouge">SHAP</code>: For model explainability and feature attribution</li> </ul> <h4 id="reference">Reference</h4> <p>[1] James, Gareth, et al. An introduction to statistical learning. Vol. 112. No. 1. New York: springer, 2013.</p> <p>[2] <a href="https://scikit-learn.org/stable/modules/ensemble.html">Scikit-learn documentation</a></p>]]></content><author><name></name></author><category term="ML learning blogs"/><category term="Classical ML"/><summary type="html"><![CDATA[ML blogs - Classical ML 2]]></summary></entry><entry><title type="html">L1 and L2 Regularization in ML</title><link href="https://yuxinshen233.github.io/blog/2025/ML1_L1-L2-regularization/" rel="alternate" type="text/html" title="L1 and L2 Regularization in ML"/><published>2025-02-28T00:00:00+00:00</published><updated>2025-02-28T00:00:00+00:00</updated><id>https://yuxinshen233.github.io/blog/2025/ML1_L1-L2-regularization</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2025/ML1_L1-L2-regularization/"><![CDATA[<h2 id="why-do-we-need-regularization">Why Do We Need Regularization?</h2> <p>Regularization is a technique used in machine learning to prevent overfitting. Overfitting occurs when a model learns the noise in the training data instead of capturing the underlying pattern, leading to poor generalization on unseen data. Regularization techniques add constraints or penalties to the model to improve its ability to generalize.</p> <h3 id="how-does-overfitting-happen">How Does Overfitting Happen?</h3> <ul> <li><strong>High model complexity:</strong> Too many parameters can lead to memorization rather than learning meaningful patterns.</li> <li><strong>Insufficient training data:</strong> Small datasets may cause models to fit noise rather than actual trends.</li> <li><strong>Noisy data:</strong> Irrelevant or redundant features can mislead the model.</li> </ul> <h2 id="what-is-l1-and-l2-regularization">What is L1 and L2 Regularization?</h2> <p>L1 and L2 regularization are two common approaches to prevent overfitting by modifying the loss function of a model:</p> <h4 id="l1-regularization-lasso-regression">L1 Regularization (Lasso Regression)</h4> <p>L1 regularization adds the <strong>absolute</strong> value of the coefficients to the loss function: \(Loss = \text{MSE} + \lambda \sum |w_i|\)</p> <ul> <li>Encourages sparsity by forcing some coefficients to be exactly zero.</li> <li>Useful for feature selection as it removes irrelevant features.</li> <li>Can lead to better interpretability.</li> </ul> <h4 id="l2-regularization-ridge-regression">L2 Regularization (Ridge Regression)</h4> <p>L2 regularization adds the <strong>squared</strong> value of the coefficients to the loss function:</p> \[Loss = \text{MSE} + \lambda\sum w_i^2\] <ul> <li>Penalizes large weights but does not shrink them to zero.</li> <li>Helps in reducing multicollinearity (high correlation between features).</li> <li>Generally leads to better stability and prevents large parameter values.</li> </ul> <h4 id="elastic-net-regularization">Elastic Net Regularization</h4> <p>Elastic Net combines both L1 and L2 penalties: \(Loss = \text{MSE} + \lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2\)</p> <ul> <li>Balances between feature selection (L1) and weight shrinkage (L2).</li> <li>Useful when there are correlated features.</li> <li>Often preferred in practical applications to get the best of both worlds.</li> </ul> <h3 id="why-is-l2-regularization-useful">Why is L2 regularization useful?</h3> <p>L1 regularization encourages sparsity and reduces model complexity. L2 regularization only penalizes large weights but does not shrink them to zero. How does L2 prevent overfitting?</p> <ul> <li>When weights are large, small changes in input features can result in disproportionately large changes in predictions, making the model highly sensitive to noise (overfitting). By shrinking weights, L2 regularization forces the model to be more stable and less sensitive to small fluctuations in the training data.</li> <li>L2 distributes the impact across multiple features, reducing the risk of the model relying too much on noise in individual features.</li> <li>In cases where features are highly correlated, the model may assign arbitrarily large positive and negative weights to compensate for these correlations, leading to instability. L2 regularization reduces this effect by shrinking the coefficients and stabilizing the model.</li> </ul> <h4 id="summary">Summary</h4> <p>L2 does not force weights to zero but shrinks them, leading to a more stable and generalizable model. By preventing excessively large weights, L2 reduces sensitivity to noise and avoids overfitting. It helps with multicollinearity and distributes learning across multiple features.</p> <h2 id="conclusion">Conclusion</h2> <p>Both L1 and L2 regularization help mitigate overfitting, but they have different effects on model complexity and feature selection. L1 is ideal when feature selection is needed, while L2 is better for stable and well-distributed parameter values. In practice, <strong>Elastic Net</strong>, a combination of L1 and L2 is often used to balance both effects.</p> <p>By understanding these techniques, you can make informed decisions when designing ML models and improving generalization performance!</p>]]></content><author><name></name></author><category term="ML learning blogs"/><category term="Classical ML"/><summary type="html"><![CDATA[ML blogs - Classical ML 1]]></summary></entry><entry><title type="html">Improving the generalization of protein expression models with mechanistic sequence information</title><link href="https://yuxinshen233.github.io/blog/2025/NARpaper/" rel="alternate" type="text/html" title="Improving the generalization of protein expression models with mechanistic sequence information"/><published>2025-02-25T00:00:00+00:00</published><updated>2025-02-25T00:00:00+00:00</updated><id>https://yuxinshen233.github.io/blog/2025/NARpaper</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2025/NARpaper/"><![CDATA[<p>Excited to share our latest work on improving the generalization of protein expression models, which has just been online in Nucleic Acids Research!</p> <p><a href="https://academic.oup.com/nar/article/53/3/gkaf020/7985285">Check the paper</a> or <a href="https://github.com/YuxinShen233/MF_for_generalization">Check the repo</a>.</p> <p>🧬 We show that integrating mechanistic sequence features—such as mRNA stability, codon usage, and peptide properties—can enhance model generalization, improving predictive accuracy for novel sequences.</p> <p>🖥️ We explore multiple strategies to combine mechanistic features with standard encodings, including feature stacking, ensemble stacking, and geometric stacking with graph neural networks.</p> <p>💡 This work highlights the importance of domain knowledge and feature engineering in ML-driven sequence design, offering new insights for applications in synthetic biology and strain engineering.</p> <p>Huge thanks to my supervisors, Dr Diego Oyarzún and Prof Grzegorz Kudla for their guidance and support throughout this project!</p>]]></content><author><name></name></author><category term="research"/><category term="DNA representation"/><category term="Graph Neural Network"/><category term="AI and Biology"/><summary type="html"><![CDATA[Paper accepted in NAR]]></summary></entry><entry><title type="html">A Turing Institute Interest Group seminar series</title><link href="https://yuxinshen233.github.io/blog/2024/DCBTuring/" rel="alternate" type="text/html" title="A Turing Institute Interest Group seminar series"/><published>2024-10-26T00:00:00+01:00</published><updated>2024-10-26T00:00:00+01:00</updated><id>https://yuxinshen233.github.io/blog/2024/DCBTuring</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2024/DCBTuring/"><![CDATA[<h4 id="abstract">Abstract</h4> <p>I am co-organising a Turing Institute Interest Group seminar series on <a href="https://www.turing.ac.uk/research/interest-groups/data-centric-biological-design-and-engineering">Data-Centric Biological Design &amp; Engineering</a>. See the <a href="https://data-centric-biodesign-tig.github.io">website</a> for more information about schedules, speakers and the zoom link.</p> <h4 id="topics">Topics</h4> <p>Our work spans several focus areas within engineering biology and artificial intelligence. The following topics capture the main themes we explore in our sessions and discussions:</p> <h5 id="protein-design-and-genotype-phenotype-mapping">Protein Design and Genotype-Phenotype Mapping</h5> <p>This session explores the forefront of protein design and genotype-phenotype mapping. We focus on how computational models and machine learning techniques are being used to predict protein structures and functions from genetic information. The session will highlight innovative applications in drug discovery, synthetic biology, and personalized medicine, demonstrating the potential to design proteins with specific functions and to understand the genetic basis of phenotypic traits.</p> <h5 id="automation-of-biological-systems">Automation of Biological Systems</h5> <p>This session focuses on the automation of biological systems, showcasing the integration of robotics, artificial intelligence, and high-throughput screening technologies. We will discuss how these automated systems are transforming biological research by accelerating experimental processes, improving reproducibility, and enabling large-scale studies. Attendees will gain insights into the latest developments in laboratory automation and how these technologies are being utilized to streamline complex biological workflows, from sample preparation to data analysis.</p> <h5 id="physically-informed-biodesign">Physically-informed Biodesign</h5> <p>Physically-informed biodesign leverages principles from physics and engineering to create robust and efficient biological systems. This session delves into how physical models and simulations are used to inform the design and optimization of biological processes and materials. Topics include the application of biomechanics, fluid dynamics, and materials science to biodesign, providing a comprehensive overview of how interdisciplinary approaches are being used to solve complex biological problems and develop new biotechnologies.</p> <h5 id="genomics-and-microbial-engineering-with-big-data">Genomics and Microbial Engineering with Big Data</h5> <p>Our final session focuses on genomics and microbial engineering, emphasizing the role of big data in these fields. We will discuss how large-scale genomic data sets are being analyzed to uncover new insights into microbial functions, interactions, and ecosystems. This session covers the use of advanced data analytics, bioinformatics, and machine learning techniques in genomics research, demonstrating how these tools are driving discoveries in microbial engineering, biotechnology, and environmental microbiology.</p>]]></content><author><name></name></author><category term="activities"/><category term="AI and Biology"/><summary type="html"><![CDATA[co-organising a seminar series]]></summary></entry><entry><title type="html">BOPE-GPT, Preference Exploration with the curious AI chemist</title><link href="https://yuxinshen233.github.io/blog/2024/Bayesian_optimization_hackathon/" rel="alternate" type="text/html" title="BOPE-GPT, Preference Exploration with the curious AI chemist"/><published>2024-03-29T00:00:00+00:00</published><updated>2024-03-29T00:00:00+00:00</updated><id>https://yuxinshen233.github.io/blog/2024/Bayesian_optimization_hackathon</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2024/Bayesian_optimization_hackathon/"><![CDATA[<p>For the Bayesian Optimization Hackathon for Chemistry and Materials (27-28 March 2024), our team finished and presented the project on BOPE-GPT, Preference Exploration with the curious AI chemist.</p> <p>Bayesian optimization with preference exploration (BOPE) is designed for Bayesian optimization of expensive-to-evaluate experiments, where the response surface function of the experiment generates vector-valued outcomes over which a decision-maker (DM) has preferences. In this project, we combined the BOPE pipeline with a language model, using the language model as the decision-maker. We worked on a multi-pbjective optimization problem (optimal production of the Fischer-Tropsch reaction) with this pipeline. Furthermore, we built an app for easy LLM-based BOPE.</p> <p>I mainly worked on the data preprocessing and Bayesian optimization pipeline (Single-objective BO, Multi-objective BO and BOPE) building.</p> <p>See <a href="https://ac-bo-hackathon.github.io/projects/project-16-bope-gpt/">project description</a> and our <a href="https://github.com/AC-BO-Hackathon/BOPE-GPT/tree/main">Git-hub page</a> for more details.</p>]]></content><author><name></name></author><category term="activities"/><category term="Bayesian Optimization"/><category term="LLM"/><summary type="html"><![CDATA[Bayesian Optimization Hackathon for Chemistry and Materials]]></summary></entry><entry><title type="html">Impact of DNA representation on the sequence-to-expression models</title><link href="https://yuxinshen233.github.io/blog/2024/EMBL/" rel="alternate" type="text/html" title="Impact of DNA representation on the sequence-to-expression models"/><published>2024-03-14T00:00:00+00:00</published><updated>2024-03-14T00:00:00+00:00</updated><id>https://yuxinshen233.github.io/blog/2024/EMBL</id><content type="html" xml:base="https://yuxinshen233.github.io/blog/2024/EMBL/"><![CDATA[<p>I gave a flash talk and the poster presentation at EMBL Symposium: AI and Biology. The topic is on DNA mechanistic representation for sequence-to-expression modelling.</p> <p>See <a href="/assets/pdf/Flashtalk_EMBL.pdf">slides</a> and <a href="/assets/pdf/Poster_EMBL.pdf">poster</a> for more details.</p>]]></content><author><name></name></author><category term="research"/><category term="DNA representation"/><category term="Graph Neural Network"/><category term="AI and Biology"/><summary type="html"><![CDATA[Flash talk and poster at EMBL Symposium]]></summary></entry></feed>